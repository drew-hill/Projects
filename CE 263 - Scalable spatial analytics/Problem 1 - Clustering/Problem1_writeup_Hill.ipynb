{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L. Drew Hill\n",
    "<br>\n",
    "CE263 Problem # 1\n",
    "<br>\n",
    "September 15, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "<br>\n",
    "A series of k-means and MiniBatch k-means processes were applied to the Latitude (km) and Longitude (km) data of 100,000 tweets randomly selected from the 1 million tweets provided to the class. Cooridnates were converted to relative KM value to allow for easier interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## randomly sample 100k datapoints from the list of json files ('tweets')\n",
    "# create index that randomly sample the data index 'n' times\n",
    "k = 100000\n",
    "random_index = random.sample(range(len(tweets)), k)\n",
    "\n",
    "# reorder the index, then take each indexed object from the list\n",
    "dt = [tweets[i] for i in sorted(random_index)]\n",
    "\n",
    "# convert list of dictionaries to data frame\n",
    "df = pd.DataFrame(dt)\n",
    "\n",
    "# make time a datetime object\n",
    "datetimer = lambda x: parse(time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(x,'%a %b %d %H:%M:%S +0000 %Y')))\n",
    "df['timeStamp'] = df['timeStamp'].apply(datetimer)\n",
    "\n",
    "# make numeric datetime \n",
    "df['timeNum']=df['timeStamp']\n",
    "numericizer = lambda x: time.mktime(x.timetuple())\n",
    "df['timeNum'] = df['timeNum'].apply(numericizer)\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "        df.at[i,'lat_km'] = df.at[i,'lat'] * 89.7\n",
    "        df.at[i,'lng_km'] = df.at[i,'lng'] * 112.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A batch size of 1%, or 1000 tweets, was selected for the MiniBatch test. Processing for 100 clusters (k=100) required 10.9 seconds for the standard k-means routine, but only 0.5 seconds for a Mini Batch k-means. I experimented with the computational/processing load of each routine by running the k-means process at k = [2, 100, 1000, 2000, 10000] and the Mini Batch k-means at k = [2, 1000, 5000, 10000, 15000, and 20000] (note: the increased performance of the MiniBatch test at lower values of k allowed me to test a wider range within a reasonable amount of time). Neither routine appeared to be constrained by Memory-- both consistently occupied ~ 1gb of ram along all values of k. Processing time, however, quickly became unbearably slow (Figure 1). \n",
    "\n",
    "![kmeans](kmeans_k_plot.pdf)\n",
    "![minibatch](MiniBatch_k_plot.pdf)\n",
    "** Figure 1 ** Time (s) plotted as a function of k (# of clusters) for k-means (top) and MiniBatch (bottom) algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "######## Clustering with k-means example\n",
    "\n",
    "# Subset dataframe to lat (km) and lng (km)\n",
    "X = df[[7,8]]\n",
    "# set cluster number\n",
    "n = 100\n",
    "## initialize with K-means++, a good way of speeding up convergence\n",
    "k_means = KMeans(init='k-means++', n_clusters= n, n_init=10)\n",
    "## record the current time\n",
    "t_km = time.time()\n",
    "# start clustering! using only lat and long (columns 2 and 3)\n",
    "k_means.fit(X)\n",
    "# results\n",
    "k_means_labels = k_means.labels_\n",
    "k_means_cluster_centers = k_means.cluster_centers_\n",
    "k_means_labels_unique = np.unique(k_means_labels)\n",
    "ft = (k_means_labels, k_means_cluster_centers, k_means_labels_unique)\n",
    "\n",
    "# get the time to finish clustering\n",
    "t_fin_km = time.time() - t_km\n",
    "print(t_fin_km)\n",
    "\n",
    "## Keep track of metrics as I iterate through values of \"k\"\n",
    "df_k_n = np.array([2.0,100.0,1000.0,2000.0,10000.0])\n",
    "df_k_time = np.array([.1,10.9,176.9,329,1349.3])\n",
    "df_k_ram = [974, 980,975]\n",
    "\n",
    "df_time = pd.DataFrame({'n':df_k_n, 'sec': df_k_time})\n",
    "\n",
    "plt.plot(df_time['n'],df_time['sec'])\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('k (# Clusters)')\n",
    "plt.annotate('k_max', xy=(4300, 600), xytext=(2500, 1000),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.1),\n",
    "            )\n",
    "plt.title('K Means')\n",
    "\n",
    "# save figure\n",
    "pylab.savefig('kmeans_k_plot.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "######## Clustering with MiniBatch example\n",
    "\n",
    "# Subset dataframe to lat (km) and lng (km)\n",
    "X = df[[7,8]]\n",
    "\n",
    "# number of clusters\n",
    "n= 2\n",
    "batch_size = 1000\n",
    "\n",
    "mbk = MiniBatchKMeans(init='k-means++', n_clusters = n, batch_size=batch_size,\n",
    "                      n_init=10, max_no_improvement=10, verbose=0)\n",
    "t0 = time.time()\n",
    "mbk.fit(X)\n",
    "t_mini_batch = time.time() - t0\n",
    "mbk_means_labels = mbk.labels_\n",
    "mbk_means_cluster_centers = mbk.cluster_centers_\n",
    "mbk_means_labels_unique = np.unique(mbk_means_labels)\n",
    "\n",
    "print(t_mini_batch)\n",
    "\n",
    "## Keep track of metrics as I iterate through values of \"k\"\n",
    "df_mb_n = [2,100,1000,5000,10000,15000,20000]\n",
    "df_mb_time = [0.05697,0.5,5.74,63.8,245.4,507.98,894.1]\n",
    "df_mb_ram = [975,974, 990,990,990,1000,1000]\n",
    "\n",
    "df_time_mb = pd.DataFrame({'n':df_mb_n, 'sec': df_mb_time, 'mb': df_mb_ram})\n",
    "\n",
    "## Plot metrics\n",
    "plt.plot(df_time_mb['n'],df_time_mb['sec'])\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('k (# Clusters)')\n",
    "plt.title('MiniBatch')\n",
    "plt.annotate('k_max', xy=(16500, 600), xytext=(10000, 800),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.1),)\n",
    "## Save plot             \n",
    "pylab.savefig('MiniBatch_k_plot.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this assignment, I set a k_max for each routined as determined by my computer's performance of each routine. Based on my own impatience, I set a time limitation of 600 seconds, which resulted in a k-means k_max of ~ 4500 and a MiniBatch k-means k_max of about 16500. This was determined graphically by plotting the results of the aforementioned expirment (Figure 1). I believe this performance bottleneck is related to the number of mathematical computations required to calculate the distance from each point to each centroid. As the number of clusters increase, so too do the number of centroids. This causes a linear increase in computation time in the k-means test, which calculates this distance for every point until optimal cluster centroids are established and, ultimately, increases computational time until it is unbearable (> 600 s). However, because the Mini Batch test takes random cluster samples (batches), the number of distance calculations is substantially smaller at lower values of k. As the number of clusters increases, I imagine the permutations of 'closest cluster' increase exponentially, causing a non-linear increase in distance calculations as k increases. I believe this may be why MiniBatch becomes increasingly bottle-necked by performance as k increases. I believe ram is barely affected by either of these routines, because distances can be stored in very efficient arrays or tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the other two routines, DBSCAN was a major memory hog. When running DBSCAN on 100,000 randomly selected tweets with a minimum sample number of 100 and ε of 10 took about 15.5 seconds but consumed over 5GB of my system's 16GB of ram according to Activity Monitor. With a bit of experimentation at ε = [0.5, 1, 10, 11, 20], I determined the ε that would produce 100 clusters is approximately ε = 1 (Figure 2), with a corresponding processing time of 1.8 seconds.\n",
    "\n",
    "![DBSCAN](DBSCAN_eps_plot.pdf)\n",
    "** Figure 2 ** Time (s), RAM use (GB), and k (# of clusters) plotted as a function of ε for several DBSCAN runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "########  DBSCAN example\n",
    "\n",
    "t_db = time.time()\n",
    "\n",
    "db = DBSCAN(eps=1, min_samples=100).fit(X)\n",
    "\n",
    "t_fin_db = time.time() - t_db\n",
    "\n",
    "## results\n",
    "db_labels = db.labels_\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "db_labels_unique = np.unique(db_labels)\n",
    "print(t_fin_db)\n",
    "print(len(db_labels_unique))\n",
    "\n",
    "## Keep track of metrics as I iterate through EPS values\n",
    "df_db_eps = [0.5,1.0,10.0,11.0,20.0]\n",
    "df_db_time = [1.3,1.96,15.5,16.6,67.86]\n",
    "df_db_ram = [.9 ,1.000,5.330,7.990,18.000]\n",
    "df_db_k = [107.0,102.0,31.0,22.0,11.0]\n",
    "\n",
    "df_time_db = pd.DataFrame({'eps':df_db_eps, 'sec': df_db_time, 'gb': df_db_ram, 'k':df_db_k})\n",
    "\n",
    "## Plot metrics\n",
    "plt.plot(df_time_db['eps'],df_time_db['sec'], label = 'Time (s)')\n",
    "plt.plot(df_time_db['eps'],df_time_db['gb'], label = 'RAM (GB)')\n",
    "plt.plot(df_time_db['eps'],df_time_db['k'], label = 'k')\n",
    "plt.ylabel('Time (s), RAM (GB), and k')\n",
    "plt.xlabel('EPS')\n",
    "pylab.legend(loc='upper right')\n",
    "\n",
    "# save plot\n",
    "pylab.savefig('DBSCAN_eps_plot.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "As shown in Figure 1, processing time was plotted as a function of k for both the k-means (k = [2, 100, 1000, 2000, 10000]) and MiniBatch (k = [2, 1000, 5000, 10000, 15000, and 20000]) routines. Both produced a relatively linear relationship between time and k.\n",
    "<br>\n",
    "<br>\n",
    "The relationship between time and sample size was also examined where n = [100, 1000, 10000, 100000] with a fixed k of 100 for the k-means and MiniBatch routines, a fixed batch size of 1% of sample size for the MiniBatch routine, and a fixed ε of 1 (corresponding to production of about 100 clusters under a sample size of 100000 as previously demonstrated) and a fixed MinPts of 100 for the DBSCAN routine. The results are shown in Figure 3. By far, the fastest of all routines under their specific setups was MiniBatch, though DBSCAN came in at a close second. K-means was considerably slower (roughly ~ 10 times).\n",
    "<br>\n",
    "<br>\n",
    "Linear curves were fit to the time ~ n relationship of each routine, and then extrapolated to estimate the amount of processing time required at n = 1 million under the previously stated conditions.\n",
    "* K-Means: Time ~ 0.00011982 * n (r2 = 1.00)\n",
    "    * Time ~ 0.00011982 * 1000000 \n",
    "    * Time ~ 120 seconds\n",
    "* MiniBatch: Time ~ 3.256e-06 * n (r2 = 0.98)\n",
    "    * Time ~ 3.3 seconds\n",
    "* DBSCAN: Time ~ 2.0069 e-05 * n (r2 = 1.00)\n",
    "    * Time ~ 20.1 seconds\n",
    "    \n",
    "\n",
    "![kMeans](kmeans_n_plot.pdf)\n",
    "![MiniBatch](minibatch_n_plot.pdf)\n",
    "** Figure 3 ** Time (s) plotted as a function of sample size for several k-means and Mini Batch k-means runs.\n",
    "\n",
    "![DBSCAN](DBSCAN_n_plot.pdf)\n",
    "** Figure 4 ** Time (s) plotted as a function of sample size at ε=1 for several DBSCAN runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function to create random samples of number \"i\" and convert lat to km\n",
    "for i in [100, 1000, 10000, 50000, 100000]:\n",
    "    random_index = random.sample(range(len(tweets)), i)\n",
    "    globals()['df_%s' % i] = [tweets[i] for i in sorted(random_index)]\n",
    "    for x in range(0,i):\n",
    "        globals()['df_%s' % i][x]['lat_km'] = globals()['df_%s' % i][x]['lat'] * 89.7\n",
    "        globals()['df_%s' % i][x]['lng_km'] = globals()['df_%s' % i][x]['lng'] * 112.7\n",
    "    globals()['df_%s' % i] = pd.DataFrame(globals()['df_%s' % i])\n",
    "\n",
    "\n",
    "################################\n",
    "###### k-means example\n",
    "\n",
    "# set cluster number\n",
    "k = 100\n",
    "\n",
    "# set dataset to df_n and subset to include only lat (km) and lon (km)\n",
    "X = df_100000[[2,4]]\n",
    "\n",
    "k_means = KMeans(init='k-means++', n_clusters= k, n_init=10)\n",
    "t_km = time.time()\n",
    "k_means.fit(X)\n",
    "t_fin_km = time.time() - t_km\n",
    "\n",
    "print(t_fin_km)\n",
    "\n",
    "df_k_n = np.array([100, 1000, 10000, 100000])\n",
    "df_k_time = np.array([ 0.12 , 0.275 , 1.48 , 12.15])\n",
    "\n",
    "df_time = pd.DataFrame({'n':df_k_n, 'sec': df_k_time})\n",
    "\n",
    "plt.plot(df_time['n'],df_time['sec'])\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('n (Sample Size)')\n",
    "plt.title('K Means')\n",
    "\n",
    "\n",
    "pylab.savefig('kmeans_n_plot.pdf')\n",
    "\n",
    "# fit curve to data\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(df_k_n.reshape(-1,1) ,df_k_time.reshape(-1,1))\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "print('Vriance score: %.2f' % regr.score(df_k_n.reshape(-1,1) ,df_k_time.reshape(-1,1)))\n",
    "\n",
    "\n",
    "################################\n",
    "###### MiniBatch example\n",
    "\n",
    "# set dataset to df_n and subset to include only lat (km) and lon (km)\n",
    "X = df_100000[[2,4]]\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "# number of clusters\n",
    "k= 100\n",
    "\n",
    "mbk = MiniBatchKMeans(init='k-means++', n_clusters = k, batch_size=batch_size,\n",
    "                      n_init=10, max_no_improvement=10, verbose=0)\n",
    "t0 = time.time()\n",
    "mbk.fit(X)\n",
    "t_mini_batch = time.time() - t0\n",
    "mbk_means_labels = mbk.labels_\n",
    "mbk_means_cluster_centers = mbk.cluster_centers_\n",
    "mbk_means_labels_unique = np.unique(mbk_means_labels)\n",
    "\n",
    "print(t_mini_batch)\n",
    "\n",
    "df_mb_n = np.array([100, 1000, 10000 ,100000])\n",
    "df_mb_time = np.array([ 0.164, 0.173, 0.232 ,0.50])\n",
    "\n",
    "df_time_mb = pd.DataFrame({'n':df_mb_n, 'sec': df_mb_time})\n",
    "\n",
    "plt.plot(df_time_mb['n'],df_time_mb['sec'])\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('n (Sample Size)')\n",
    "pylab.ylim([0,.50])            \n",
    "pylab.xlim([0,100000])\n",
    "    \n",
    "plt.title('Mini Batch')\n",
    "pylab.savefig('minibatch_n_plot.pdf')\n",
    "\n",
    "# fit curve to data\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(df_mb_n.reshape(-1,1) ,df_mb_time.reshape(-1,1))\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "print('Vriance score: %.2f' % regr.score(df_mb_n.reshape(-1,1) ,df_mb_time.reshape(-1,1)))\n",
    "\n",
    "\n",
    "################################\n",
    "###### DBSCAN example\n",
    "\n",
    "# set dataset to df_n and subset to include only lat (km) and lon (km)\n",
    "X = df_100000[[2,4]]\n",
    "\n",
    "## DBSCAN\n",
    "t_db = time.time()\n",
    "db = DBSCAN(eps=1, min_samples=100).fit(X)\n",
    "t_fin_db = time.time() - t_db\n",
    "\n",
    "## DBSCAN Results\n",
    "db_labels = db.labels_\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "db_labels_unique = np.unique(db_labels)\n",
    "\n",
    "print(t_fin_db)\n",
    "\n",
    "# keep track of metrics\n",
    "df_db_n = np.array([100, 1000, 10000, 100000])\n",
    "df_db_time = np.array([0.0019,0.0065,0.0714,1.97])\n",
    "\n",
    "df_time_db = pd.DataFrame({'n':df_db_n, 'sec': df_db_time})\n",
    "\n",
    "# plot metrics\n",
    "plt.plot(df_time_db['n'],df_time_db['sec'], label = 'Time (s)')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('Sample Size')\n",
    "pylab.savefig('DBSCAN_n_plot.pdf')\n",
    "\n",
    "# fit curve to data\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(df_db_n.reshape(-1,1) ,df_db_time.reshape(-1,1))\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "print('Vriance score: %.2f' % regr.score(df_db_n.reshape(-1,1) ,df_db_time.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "I approached this problem using the hierarchical method suggested by Alexei. In general, the approach followed these basic steps:\n",
    "    1. Full dataset is run through MiniBatch processing\n",
    "    2. Each MiniBatch cluster is run through DBSCAN with eps = 100m, and min_samples = 100. This was intended to simulate a cluster core of at least 100 samples within a radius of 100m.\n",
    "\n",
    "Considering the computational processing and memory limitations analyzed in Part 2, I iterated through the MiniBatch step with k = [10, 25, 35, 50, 75, 100, 200] and a batch size of 1% of the sample (10000). These tests went very quickly, and so I pushed the batch size to 10% (100000) to produce the following computational times at k = [10, 25, 35, 50, 75, 100, 200], t = [2.51, 5.96, 8.40, 12.35, 28.00, 61.80] in seconds. A tolerable sweet spot seemed to be less than a minute, and so I limited my k options (at a batch size of 100000) to 10-100. This also seemed to produce a reasonable sample size for DBSCAN, with a minimum of 1000 datapoints at k= 100 clusters, if divided evenly among the samples.\n",
    "<br>\n",
    "<br>\n",
    "The MiniBatch output produced by the k range of 10-100 was then processed through DBSCAN at EPS_100 = 0.1 and min_samples = 100. For timing considerations, a truncated set was used, k = [10, 25, 50, 100]. At each k, the following was observed:\n",
    "* 10  MiniBatch: 1607 assigned clusters, 22.82 sec\n",
    "* 25  MiniBatch: 1604 assigned clusters, 28.27 sec\n",
    "* 50  MiniBatch: 1605 assigned clusters, 37.89 sec\n",
    "* 100 MiniBatch: 1606 assigned clusters, 58.60 sec\n",
    "* 200 MiniBatch: 1614 assigned clusters, 104.67 sec (included for fun/comparison, even though out of my pre-defined range of k)\n",
    "\n",
    "Loading on RAM did not exceed approximately 2.3 GB, and so was not a major consideration at this range of parameters.\n",
    "<br>\n",
    "<br>\n",
    "With these considerations in mind, I believe the following hierarchical clustering parameters to be optimal: a MiniBatch with k = 10 and a batch size of 100,000, followed by DBSCAN on each resulting cluster with EPS_100 = 0.1 and min_samples = 100. This set produced almost exactly the same number of assigned clusters (1607) as all larger k values examined at this batch size. (This excludes any unassigned clusters labelled \"-1\" during the DBSCAN routine.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit Visualization\n",
    "I quasi-randomly decided to examine a cluster named 0_121 -- which means it was sub-cluster number 121 as determined by DBSCAN of the first MiniBatch cluster (0). This cluster falls within the latitudes of 37.795 to 37.803 (median: 37.7998) and the longitudes of -122.444 to -122.433 (median: -122.437939). The region it covers spans about 1.21 km across and about 0.64 km up and down the map (using the rough coordinate conversions suggested in the assignment). Tweet coordinates ar eplotted in Figure 5.\n",
    "<br>\n",
    "<br>\n",
    "Based on these coordinates, the cluster is likely a representation of the Marina District in San Francisco. Based on the shape of the cluster, I would posit these tweets come from the Lombard St / Chestnut St @ Filmore Street shopping and dining district in the Marina (Figure 6).\n",
    "<br>\n",
    "<br>\n",
    "Examining the text of the tweets in this cluster, one major theme emerges: celebration, with subtopics in drinking and eating. This must be a dining region. Throughout the range of dates/times these tweets cover (Sep 12 - Oct 5), a good number of sports events appear to be commented upon as well. I believe the linguistic content of the tweets confirms that this cluster is, indeed, home to many Marina Bros.\n",
    "\n",
    "![CLusterPlot](Cluster_plot.pdf)\n",
    "** Figure 5 ** Cluster \"0_121\" plotted in terms of latitude and longitude.\n",
    "![Marina](Marina.pdf)\n",
    "** Figure 6 ** The central Lat and Long coordinates (the red pin) of cluster \"0_121\" as shown on Google Maps.\n",
    "![wordle](wordle.png)\n",
    "** Figure 7 ** Some of the most commonly used words tweeted at this location. Size indicates relative frequency. Created at Wordle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Standalone Script for Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import time,datetime\n",
    "import pylab\n",
    "import json\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans, DBSCAN, MiniBatchKMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from dateutil.parser import parse\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "# setting to show the plots in line in the notebook\n",
    "%matplotlib inline  \n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load Tweet Dataset\n",
    "with open('tweets_1M.json','r') as f:\n",
    "    tweets=json.load(f)\n",
    "\n",
    "# Convert tweets to data frame\n",
    "twits = pd.DataFrame(tweets)\n",
    "\n",
    "# convert lat long\n",
    "for i in range(0,len(twits)):\n",
    "        twits.at[i,'lat_km'] = twits.at[i,'lat'] * 89.7\n",
    "        twits.at[i,'lng_km'] = twits.at[i,'lng'] * 112.7\n",
    "    \n",
    "# confirm column values\n",
    "twits.columns.values\n",
    "\n",
    "# subset to include only lat (km) and long (km) columns\n",
    "X = twits[[6,7]]\n",
    "\n",
    "################################\n",
    "##### Inputs for experimentation\n",
    "################################\n",
    "# change with each iteration\n",
    "n= 100\n",
    "batch_size = 100000\n",
    "\n",
    "# start timer\n",
    "t_total_0 = time.time()\n",
    "\n",
    "############\n",
    "## MiniBatch K Means\n",
    "mbk = MiniBatchKMeans(init='k-means++', n_clusters = n, batch_size=batch_size,\n",
    "                      n_init=10, max_no_improvement=10, verbose=0)\n",
    "t0 = time.time()\n",
    "mbk.fit(X)\n",
    "t_mini_batch = time.time() - t0\n",
    "mbk_means_labels = mbk.labels_\n",
    "mbk_means_cluster_centers = mbk.cluster_centers_\n",
    "mbk_means_labels_unique = np.unique(mbk_means_labels)\n",
    "\n",
    "print(t_mini_batch)\n",
    "\n",
    "## Add labels back to the original dataset\n",
    "twits['label']= mbk_means_labels\n",
    "# twits for each MiniBatch cluster, then remove label\n",
    "for i in range(1,n):\n",
    "    globals()['t%s' % i] = twits[['lat_km','lng_km','label']].query('label ==  %s' % i)[['lat_km','lng_km']]\n",
    "\n",
    "# keep track of metrics\n",
    "MB_hier_k = np.array([10, 25, 35, 50, 75, 100, 200])\n",
    "MB_hier_sec = np.array([2.51, 5.96, 8.40, 12.35, 20.15, 28.00, 61.80])    \n",
    "\n",
    "################\n",
    "## DBSCAN\n",
    "\n",
    "# function to run dbscan on twits by 'label' value; store new dataframes in list\n",
    "df_list = {}\n",
    "for i in range(0, n ):\n",
    "    # split/subset by MiniBatch label\n",
    "    df_interim = twits[['lat_km','lng_km','label']].query('label == %s' % i)\n",
    "    # remove label column\n",
    "    df_interim = df_interim[['lat_km','lng_km']]\n",
    "    # run DBSCAN\n",
    "    db = DBSCAN(eps=0.1, min_samples=100).fit(df_interim)\n",
    "    # create column for label and DBSCAN group\n",
    "    df_interim['label_db'] = db.labels_\n",
    "    df_interim['db_group'] = '%s' % i\n",
    "    # create a list of all dataframes, clusters 0 - (n-1)\n",
    "    df_list['df_db%s' % i] = df_interim\n",
    "\n",
    "# recombine by concatenating the df_list into one final dataframe\n",
    "df_final = pd.concat(df_list)\n",
    "\n",
    "# Paste Label and DBSCAN Group values together to create unique\n",
    "# cluster label for each row\n",
    "import functools\n",
    "def reduce_concat(x, sep=\"\"):\n",
    "    return functools.reduce(lambda x, y: str(x) + sep + str(y), x)\n",
    "\n",
    "def paste(*lists, sep=\" \", collapse=None):\n",
    "    result = map(lambda x: reduce_concat(x, sep=sep), zip(*lists))\n",
    "    if collapse is not None:\n",
    "        return reduce_concat(result, sep=collapse)\n",
    "    return list(result)\n",
    "\n",
    "df_final['cluster'] = paste(df_final.db_group,df_final.label_db, sep = \"_\")\n",
    "\n",
    "## Count all labels (a.k.a. unassigned clusters)\n",
    "print(\"Total # of Clusters:\", df_final.cluster.nunique())\n",
    "\n",
    "## Count labels, excluding \"-1\" (a.k.a. unassigned clusters)\n",
    "print(\"Total # of assigned Clusters:\" , df_final.query('label_db != -1').cluster.nunique())\n",
    "\n",
    "\n",
    "# End time\n",
    "t_total = time.time() - t_total_0\n",
    "print(\"Total runtime for MiniBatch of \",n, \"samples:\" , t_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra credit code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Convert lat/lng back\n",
    "df_final['lat'] = df_final['lat_km']/89.7\n",
    "df_final['lng'] = df_final['lng_km']/112.7\n",
    "\n",
    "## Chose a cluster\n",
    "grouped = df_final[['cluster','lat']].groupby('cluster')\n",
    "grouped.count()\n",
    "\n",
    "## I chose 0_121, as it has a fair number of samples (~1650)\n",
    "df_clust = df_final.query('cluster == \"0_121\"') \n",
    "\n",
    "### Describe lat and long\n",
    "df_clust.describe()\n",
    "\n",
    "\n",
    "### Plot it\n",
    "uniq = list(set(df_clust['cluster']))\n",
    "uniq\n",
    "# df_final['lat'].describe()\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "# determine number of unique clusters in Mill Valley area (10)\n",
    "print('Number of Clusters:',df_clust.cluster.nunique())\n",
    "uniq = list(set(df_clust['cluster']))\n",
    "# Set the color map to match the number of clusters\n",
    "z = range(1,len(uniq))\n",
    "cols = plt.get_cmap('coolwarm')\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=len(uniq))\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cols)\n",
    "\n",
    "# plot each cluster\n",
    "for i in range(len(uniq)):\n",
    "    indx = df_clust['cluster'] == uniq[i]\n",
    "    plt.scatter(df_clust['lat'], df_clust['lng'], s=20, color=scalarMap.to_rgba(i), alpha = 0.1)\n",
    "\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.title('Tweets of Cluster 0_121')\n",
    "# plt.legend(loc='center left', fancybox=True, shadow=True, bbox_to_anchor=(1,0.5))\n",
    "plt.show()\n",
    "\n",
    "# save plot\n",
    "pylab.savefig('Cluster_plot.pdf')\n",
    "\n",
    "\n",
    "### Manually examine tweets in this cluster\n",
    "## subset full tweets dataset by geographic region\n",
    "    # range: \n",
    "        # lat 37.795 to 37.803\n",
    "        # lng -122.444 to -122.433\n",
    "    \n",
    "df_marina = df.query('lat < 37.803 and lat > 37.795 and lng < -122.433 and lng > -122.444')\n",
    "df_marina[['text','timeStamp']]\n",
    "df_marina['cluster'] = str('0_121')\n",
    "\n",
    "### Look at every bit of text in this cluster\n",
    "# concatenate all text (row by row), separated by a ' '\n",
    "df_wordle= df_marina.groupby(['cluster'])['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# output that concatenated text to clipboard for pasting into Wordle\n",
    "df_wordle.to_clipboard()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
