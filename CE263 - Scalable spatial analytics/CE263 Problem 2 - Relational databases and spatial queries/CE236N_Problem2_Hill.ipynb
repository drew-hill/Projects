{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L. Drew Hill (SID 21707129)\n",
    "\n",
    "### Note: I had no partners... I didn't want to subject them to my non-engineering background.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Fiona is an OGR-based Python library that is used to read and write geometry data types like multi-layered GIS formats and zipped virtual file systems via standard Python input-output style. It can be used to convert a geometric database into a postgreSQL-compatible database.\n",
    "\n",
    "Shapely is used for the manipulation and spacial analysis in Python of planar geometric objects (which can be read and written via Fiona) that are not tied to specific data formats or coordniate systems. Shapely is based on GEOS and is very fast and efficient with planar analyses like geometric buffers, unions, intersections, and centroids.\n",
    "\n",
    "PostGIS can perform both spacial and geometric analysis and read/write geometry data types to a PostgreSQL database management system. PostGIS is based on light-weight geometries, reducing disk and memory footprints and providing considerable convenience for speedily querying large spatial databases in long-term PostgreSQL storage.\n",
    "\n",
    "Simple ad-hoc spatial data analysis could probably be done efficiently and quickly using a combination of Fiona to read/write the data and Shapely to analyze it. Longer term storage involving tidy data structure and relational storage would be best performed using PostGIS and postgreSQL, perhaps through the psycopg2 Python package if necessary. For data distribution, I would employ Fiona to format and read data out of Python in a way that would produce minimal formatting issues for folks wanting to import the data into Python, ESRI or another analysis tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task II\n",
    "\n",
    "\n",
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "import random\n",
    "from dateutil.parser import parse\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create database via psql in terminal**\n",
    "\n",
    "CREATE DATABASE task2 OWNER Lawson TEMPLATE template_postgis ENCODING utf8;\n",
    "\n",
    "** start server in shell**\n",
    "\n",
    "pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to database that I created\n",
    "try:\n",
    "    conn = \n",
    "    psycopg2.connect(\"dbname=task2 user=Lawson host=localhost password=****\")\n",
    "except:\n",
    "    print \"Cannot connect to database\"\n",
    "cur = conn.cursor()\n",
    "\n",
    "# # verify error-free execution\n",
    "# cur.execute(\"\"\"SELECT srtext FROM spatial_ref_sys WHERE srid = 32610;\"\"\")\n",
    "# rows = cur.fetchall()\n",
    "# for row in rows:\n",
    "#     print \"\", row[0]\n",
    "\n",
    "# conn.commit()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import the twitter database\n",
    "with open('tweets_1M.json','r') as f:\n",
    "    twitters=json.load(f)\n",
    "    \n",
    "## Randomly select 100,000 tweets\n",
    "k = 100000\n",
    "random_index = random.sample(range(len(twitters)), k)\n",
    "# reorder the index, then take each indexed object from the list\n",
    "tweets = [twitters[i] for i in sorted(random_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create ST_GeomFromText object for use in SQL\n",
    "for i in range(0,len(tweets)):\n",
    "    tweets[i]['geoloc'] = 'Point(%s' % tweets[i]['lng'] + \n",
    "    ' %s)' % tweets[i]['lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse datetime\n",
    "for i in range(0,len(tweets)):\n",
    "    tweets[i]['timeStamp'] = parse(tweets[i]['timeStamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # create empty geoJSON object\n",
    "# tweet_geo = {\n",
    "#     \"type\": \"FeatureCollection\",\n",
    "#     \"features\" : []\n",
    "# }\n",
    "\n",
    "# # fill geoJSON object with each tweet\n",
    "# for i in range(0,len(tweets)):\n",
    "#     twit = [\n",
    "#         {\"type\": \"Feature\",\n",
    "#             \"geometry\": {\n",
    "#                \"type\": \"Point\",\n",
    "#                \"loc\": [tweets[i]['lat'],tweets[i]['lng']]\n",
    "#                 },\n",
    "#                 \"properties\": {\n",
    "#                     \"id\": tweets[i]['id'],\n",
    "#                     \"text\": tweets[i]['text'],\n",
    "#                     \"time\": tweets[i]['timeStamp'],\n",
    "#                     \"user_id\": tweets[i]['user_id']\n",
    "#                 }\n",
    "#             }]\n",
    "#     tweet_geo['features'].append(twit)\n",
    "    \n",
    "# # # Save geo data as geoJSON\n",
    "# # with open('tweet_geo.json', 'w') as fout:\n",
    "# #     fout.write(json.dumps(tweet_geo, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## determine max length of text and thus what CHAR(n) to use\n",
    "text_len = []\n",
    "for i in range(0,len(tweets)):\n",
    "    a = [len(tweets[i]['text'])]\n",
    "    text_len.append(a)\n",
    "    \n",
    "max(text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create table in which to store tweets\n",
    "cur.execute(\"\"\"CREATE TABLE tweets (id char(50) PRIMARY KEY, \n",
    "    userid INT, loc CHAR(100), time timestamptz, text CHAR(506));\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Insert tweets-- each as a new row\n",
    "cur.executemany(\"\"\"INSERT INTO tweets(id, userid, loc, time, text) \n",
    "    VALUES (%(id)s, %(user_id)s, ST_GeomFromText(%(geoloc)s, 4326), \n",
    "    timestamptz(%(timeStamp)s), %(text)s)\"\"\", tweets)\n",
    "# commit new datatable to database\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Parts 4 & 5\n",
    "\n",
    "The census population shapefile data for counties in California were downloaded from http://www.census.gov/cgi-bin/geo/shapefiles2010/main, and inserted into my database (called \"task2\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Convert shape files into form compatible with my database in SHELL:**\n",
    "\n",
    "shp2pgsql -I -W \"latin1\" -s 4326 /Users/Lawson/Box\\ Sync/Current\\ Coursework/CE263\\ -\\ Scalable\\ Spatial\\ Analytics/Assignments/Problem\\ 2/tl_2010_06_county10/tl_2010_06_county10.shp public.ca_census_tract | psql -d task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6\n",
    "\n",
    "From the CA census shapefile, I was able to establish how many of the 100k tweets were tweeted from Contra Costa county. The \"ST_Interects\" command was used to isolate all tweet coordinate pairs inside of the shapefiles presented to PSQL, and a select command was used to ensure that only the Alameda County shapefile was presented. A count command was used to count all of these Contra-Costa-specific tweet IDs.\n",
    "\n",
    "*A total of **8,621 tweets** from my 100k subset originated from Contra Costa County*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(86L,)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculate the # of tweets in Contra Costa county\n",
    "cur.execute(\"\"\"SELECT COUNT(tweets.id) \n",
    "FROM tweets, ca_census_tract \n",
    "WHERE ST_Intersects(ca_census_tract.geom, tweets.loc) \n",
    "AND ca_census_tract.name10 = 'Contra Costa';\"\"\") \n",
    "\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part 7\n",
    "\n",
    "I next set about counting the number of tweets sent from within 100 miles outside of the Alameda county border. First, a 160,934 meter (100 mile) buffer was set around the geographic border of Alameda County. This buffer was converted into a geometry and then any tweet locations intersecting the buffer. Next, all tweets that also interesected with the geometry of the county shape (i.e. tweets coming from within the county) were excluded. All remaining tweets were counted by unique tweet id.\n",
    "\n",
    "Approximately **68,320 tweets** of the 100k randomly selected tweets were sent from within outside 100 mi of Alameda County. This makes sense considering the major metropolitan centers included within a 100 mi radius of Alameda county's borders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Calculate how many tweets are w/in 100 mi outside of Alameda county\n",
    "\n",
    "## First get all points within 100.00 m of any shapefile border\n",
    "## Second, makesure PSQL is only considering the Alameda shapefile\n",
    "## Third, exclude all points within the county polygon itself, \n",
    "## thus leaving only points  100m outside the border.\n",
    "\n",
    "cur.execute(\"\"\"SELECT COUNT(tweets.id) \n",
    "FROM tweets, ca_census_tract \n",
    "WHERE NOT ST_Intersects(ca_census_tract.geom, tweets.loc)\n",
    "AND ST_Intersects( \n",
    "ST_Buffer(ca_census_tract.geom::Geography,160934)::geometry, tweets.loc)\n",
    "AND ca_census_tract.name10 = 'Alameda';\"\"\") \n",
    "\n",
    "cur.fetchall()\n",
    "\n",
    "\n",
    "# # attempt using degrees and a wonky conversion factor\n",
    "# cur.execute(\"\"\"SELECT count(tweets.id) \n",
    "# FROM tweets, ca_census_tract \n",
    "# WHERE NOT ST_Intersects(ca_census_tract.geom, tweets.loc)\n",
    "# AND ST_DWithin(ca_census_tract.geom, tweets.loc, 1.59) \n",
    "# AND ca_census_tract.name10 = 'Alameda' ;\"\"\") \n",
    "\n",
    "# cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create table in which to store 2010 census pop data\n",
    "cur.execute(\"\"\"CREATE TABLE ca_census \n",
    "(GEOID integer PRIMARY KEY, \n",
    "SUMLEV varchar(3), \n",
    "STATE varchar(2),\n",
    "COUNTY varchar(3),\n",
    "CBSA varchar(5),\n",
    "CSA varchar(3),\n",
    "NECTA integer,\n",
    "CNECTA integer,\n",
    "NAME varchar(30),\n",
    "POP100 numeric,\n",
    "HU100 integer,\n",
    "POP100_2000 integer,\n",
    "HU100_2000 integer,\n",
    "P001001 integer,\n",
    "P001001_2000 integer);\"\"\")\n",
    "\n",
    "# commit change to database\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create SQL command to insert CSV into table \"ca_census\":\n",
    "copy_sql_string = \"\"\" COPY ca_census FROM stdin \n",
    "WITH CSV HEADER DELIMITER as ',' \"\"\"\n",
    "\n",
    "# open csv file\n",
    "f = open(r'/tl_2010_06_county10/ca_census.csv', 'r')\n",
    "# copy csv file\n",
    "cur.copy_expert(sql=copy_sql_string, file=f)\n",
    "# commit changes\n",
    "conn.commit()\n",
    "\n",
    "# close CSV\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 9\n",
    "\n",
    "Tweet per capita was calculated for each county using the code outlined below. Briefly, I joined the census shapefiles with population values (as matched by geoid, or county id) into a temporary table, joined that temporary table with the table of all 100k tweets, and then counted the total number of tweets divided by county population as grouped by county.\n",
    "\n",
    "These data were stored in a table along with all ocunty shapefiles. This table was used to produce a QGIS image depicting tweets per capita using a red-green heatmap-style shading, as seen below.\n",
    "\n",
    "![](TweetPerCap.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Some prep work\n",
    "\n",
    "# convert geoid10 column of census_tract to match INTEGER type of the geoid column of ca_census \n",
    "cur.execute(\"\"\"ALTER TABLE ca_census_tract ALTER COLUMN geoid10 \n",
    "TYPE integer USING (geoid10::integer);\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# rename geoid10 to match ca_census geoid\n",
    "cur.execute(\"\"\"ALTER TABLE ca_census_tract RENAME COLUMN \n",
    "    geoid10 to geoid;\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# set ca_census_tract primary key to geoid\n",
    "    # drop current key\n",
    "cur.execute(\"\"\"ALTER TABLE ca_census_tract DROP CONSTRAINT \n",
    "    ca_census_tract_pkey;\"\"\")\n",
    "    # make geoid primary key\n",
    "cur.execute(\"\"\"ALTER TABLE ca_census_tract DROP CONSTRAINT \n",
    "    ca_census_tract_pkey;\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(\"\"\"CREATE TABLE tweet_percap_final \n",
    "(geoid INT PRIMARY KEY, pop INT, geom geometry , \n",
    "    tweet_percap numeric);\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize tweets per capita for CA counties using a single query\n",
    "cur.execute(\"\"\"WITH newtable AS (\n",
    "    SELECT ca_census.geoid geoid, ca_census.pop100 pop, \n",
    "        ca_census_tract.geom geom \n",
    "    FROM ca_census, ca_census_tract\n",
    "    WHERE ca_census.geoid = ca_census_tract.geoid\n",
    "    ) \n",
    "    INSERT INTO tweet_percap_final\n",
    "    SELECT newtable.geoid, newtable.pop, newtable.geom, \n",
    "        ROUND(count(tweets.id::numeric) /newtable.pop , 7) tweet_percap\n",
    "    FROM tweets\n",
    "    JOIN newtable ON ST_Intersects(newtable.geom, tweets.loc)\n",
    "    GROUP BY newtable.geom, newtable.geoid, newtable.pop;\"\"\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ipykernel_py2]",
   "language": "python",
   "name": "conda-env-ipykernel_py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
